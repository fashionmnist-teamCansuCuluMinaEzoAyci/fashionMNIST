{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ce39f9-f00d-4f06-a3a6-c67a58909686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "LOSS_YLIM   = (0.0, 1.2)\n",
    "LOSS_YTICKS = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]\n",
    "\n",
    "ACC_YLIM    = (0.7, 1.0)\n",
    "ACC_YTICKS  = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483d0b58-b118-4699-bf37-5de29b89c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN for Fashion-MNIST \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)   # 28x28\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)   # 28x28\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea10b5fe-b367-4a72-bd07-8c4b43b623b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    # Apple Silicon GPU\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    # NVIDIA CUDA GPU\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # CPU \n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def ensure_dirs(base_dir: Path) -> tuple[Path, Path]:\n",
    "    ckpt_dir = base_dir / \"outputs\" / \"ckpt\"\n",
    "    fig_dir = base_dir / \"outputs\" / \"figures\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return ckpt_dir, fig_dir\n",
    "\n",
    "\n",
    "def unnormalize(img_tensor: torch.Tensor) -> np.ndarray:\n",
    "    mean, std = 0.2860, 0.3530\n",
    "    x = img_tensor.clone()\n",
    "    x = x * std + mean\n",
    "    x = x.clamp(0, 1)\n",
    "    return x.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def save_grid(images: np.ndarray, title: str, out_path: Path, cols: int = 4) -> None:\n",
    "    \"\"\"\n",
    "    images: (N, H, W)\n",
    "    \"\"\"\n",
    "    n = images.shape[0]\n",
    "    rows = int(np.ceil(n / cols))\n",
    "\n",
    "    plt.figure(figsize=(cols * 1.5, rows * 1.5))\n",
    "    \n",
    "    for i in range(n):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        img = images[i]\n",
    "        # normalize per-map for visibility\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e28ef3-a7bf-4de3-9253-5eb407d04f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 42\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    val_ratio: float = 0.1\n",
    "    num_workers: int = 0  \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,loader: DataLoader, device: torch.device) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validation/test evaluation loop.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    avg_loss = loss_sum / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def plot_curves(history: dict, out_path: Path) -> None:\n",
    "    epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Model-1 Loss Curve\")\n",
    "    plt.ylim(LOSS_YLIM)\n",
    "    plt.yticks(LOSS_YTICKS)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path.with_name(\"training_loss.png\"), dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model-1 Accuracy Curve\")\n",
    "    plt.ylim(ACC_YLIM)\n",
    "    plt.yticks(ACC_YTICKS)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path.with_name(\"training_accuracy.png\"), dpi=120)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec51f778-8ce8-443a-a8b7-517b52986f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(cfg: TrainConfig, base_dir: Path | None = None):\n",
    "    \n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    ckpt_dir, fig_dir = ensure_dirs(base_dir)\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.2860,), (0.3530,))])\n",
    "\n",
    "    # Dataset + split\n",
    "    full_train = datasets.FashionMNIST(\n",
    "        root=str(base_dir / \"data\"),\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    val_size = int(len(full_train) * cfg.val_ratio)\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_ds, val_ds = random_split(full_train,[train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(cfg.seed)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = FashionCNN(num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=cfg.lr,weight_decay=cfg.weight_decay)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val_acc = -1.0\n",
    "    best_path = ckpt_dir / \"model1_best.pt\"\n",
    "\n",
    "    #Training and Validation Loop\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train() #Switch model to training mode\n",
    "        total, correct = 0, 0\n",
    "        loss_sum = 0.0\n",
    "\n",
    "        for x, y in train_loader: # Inner batch loop\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "        train_loss = loss_sum / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"config\": asdict(cfg),\n",
    "                    \"val_acc\": best_val_acc,\n",
    "                },\n",
    "                best_path,\n",
    "            )\n",
    "\n",
    "    \n",
    "    plot_curves(history, fig_dir / \"training_curves.png\")\n",
    "\n",
    "    \n",
    "    meta = {\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"device\": str(device),\n",
    "        \"config\": asdict(cfg),\n",
    "    }\n",
    "    (ckpt_dir / \"train_meta.json\").write_text(\n",
    "        json.dumps(meta, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSaved best checkpoint: {best_path}\")\n",
    "    print(f\"Saved training curves to: {fig_dir}\")\n",
    "\n",
    "    return model, best_path, history, fig_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea462da7-b28a-4f93-848c-0b4328c52681",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_test_eval(base_dir: Path | None = None, ckpt_name: str = \"model1_best.pt\"):\n",
    "    \n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    _, fig_dir = ensure_dirs(base_dir)\n",
    "    ckpt_path = base_dir / \"outputs\" / \"ckpt\" / ckpt_name\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.2860,), (0.3530,))])\n",
    "\n",
    "    test_ds = datasets.FashionMNIST(\n",
    "        root=str(base_dir / \"data\"),\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=256,\n",
    "        shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    model = FashionCNN(num_classes=10).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_true_all.append(y.cpu().numpy())\n",
    "        y_pred_all.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4)\n",
    "    print(report)\n",
    "    (fig_dir / \"classification_report_model1.txt\").write_text(report, encoding=\"utf-8\")\n",
    "\n",
    "    # Confusion matrix (normalized)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    cm_norm = cm / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm_norm, cmap=\"Blues\")\n",
    "    plt.title(\"Normalized Confusion Matrix (Model 1 - PyTorch)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(10), CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(10), CLASS_NAMES)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.text(j, i, f\"{cm_norm[i, j]:.2f}\",\n",
    "                     ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = fig_dir / \"confusion_matrix_normalized_model1.png\"\n",
    "    plt.savefig(out_path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved normalized confusion matrix to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4328558b-92f4-4ecf-a5e7-9a533d9e8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_visualizations(base_dir: Path | None = None, ckpt_name: str = \"model1_best.pt\", sample_idx: int = 123):\n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    _, fig_dir = ensure_dirs(base_dir)\n",
    "    ckpt_path = base_dir / \"outputs\" / \"ckpt\" / ckpt_name\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))])\n",
    "\n",
    "    test_ds = datasets.FashionMNIST(\n",
    "        root=str(base_dir / \"data\"),\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    model = FashionCNN(num_classes=10).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    #Sample images\n",
    "    idxs = list(range(10))\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        x, y = test_ds[idx]\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(unnormalize(x), cmap=\"gray\")\n",
    "        plt.title(CLASS_NAMES[int(y)], fontsize=9)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Sample Images from Fashion-MNIST Dataset\")\n",
    "    plt.tight_layout()\n",
    "    out_path = fig_dir / \"sample_images_fashion_mnist_pytorch.png\"\n",
    "    plt.savefig(out_path, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"Saved Figure 1 to: {out_path}\")\n",
    "\n",
    "   \n",
    "    #Filters visualization (conv1)\n",
    "    w = model.conv1.weight.detach().cpu().numpy()  # (outC, inC, k, k)\n",
    "    w = w[:, 0, :, :]  # grayscale -> (outC, k, k)\n",
    "    n_filters = min(16, w.shape[0])\n",
    "    filters = w[:n_filters]\n",
    "\n",
    "    save_grid(filters, \"Learned Filters (conv1) - Model 1\", fig_dir / \"filters_conv1_model1.png\", cols=4)\n",
    "    print(f\"Saved filters to: {fig_dir / 'filters_conv1_model1.png'}\")\n",
    "\n",
    "    #Activation maps with hooks\n",
    "    activations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    def hook_fn(name):\n",
    "        def _hook(module, inp, out):\n",
    "            activations[name] = out.detach()\n",
    "        return _hook\n",
    "\n",
    "    h1 = model.conv1.register_forward_hook(hook_fn(\"conv1\"))\n",
    "    h2 = model.conv2.register_forward_hook(hook_fn(\"conv2\"))\n",
    "    h_pool = model.pool.register_forward_hook(hook_fn(\"pool\"))\n",
    "\n",
    "    x, y_true = test_ds[sample_idx]\n",
    "    x_batch = x.unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "    y_pred = int(torch.argmax(logits, dim=1).item())\n",
    "\n",
    "    \n",
    "    h1.remove()\n",
    "    h2.remove()\n",
    "    h_pool.remove()\n",
    "\n",
    "    # Sample preview\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(unnormalize(x), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\n",
    "        f\"idx={sample_idx} | True: {CLASS_NAMES[int(y_true)]} \"\n",
    "        f\"| Pred: {CLASS_NAMES[y_pred]}\"\n",
    "    )\n",
    "    sample_path = (\n",
    "        fig_dir / f\"sample_{sample_idx}_true_{int(y_true)}_pred_{y_pred}_model1.png\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sample_path, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"Saved sample preview to: {sample_path}\")\n",
    "\n",
    "    # Activation maps (1, C, H, W) -> first N channels\n",
    "    for layer_name in [\"conv1\", \"conv2\"]:\n",
    "        act = activations[layer_name][0].cpu().numpy()  # (C, H, W)\n",
    "        n_maps = min(16, act.shape[0])\n",
    "        maps = act[:n_maps]\n",
    "\n",
    "        out_path = fig_dir / f\"activation_{layer_name}_idx{sample_idx}_model1.png\"\n",
    "        save_grid(maps, f\"Activation Maps ({layer_name}) - idx {sample_idx} - Model 1\", out_path, cols=4)\n",
    "        print(f\"Saved activation maps to: {out_path}\")\n",
    "\n",
    "    act = activations[\"pool\"][0].cpu().numpy()  # (C, H, W)\n",
    "    n_maps = min(16, act.shape[0])\n",
    "    maps = act[:n_maps]\n",
    "\n",
    "    out_path = fig_dir / f\"activation_pool_idx{sample_idx}_model1.png\"\n",
    "    save_grid(\n",
    "    maps,\n",
    "    f\"Activation Maps (pool) - idx {sample_idx} - Model 1\",\n",
    "    out_path,\n",
    "    cols=4\n",
    ")\n",
    "\n",
    "    print(\"Done. Check outputs/figures/ folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c23ced8-7cbb-45a2-afe6-6d6a278535da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Epoch 01/12 | train_loss=0.4344 train_acc=0.8449 | val_loss=0.2903 val_acc=0.8978\n",
      "Epoch 02/12 | train_loss=0.2771 train_acc=0.8986 | val_loss=0.2435 val_acc=0.9092\n",
      "Epoch 03/12 | train_loss=0.2281 train_acc=0.9160 | val_loss=0.2229 val_acc=0.9193\n",
      "Epoch 04/12 | train_loss=0.1939 train_acc=0.9289 | val_loss=0.1957 val_acc=0.9278\n",
      "Epoch 05/12 | train_loss=0.1676 train_acc=0.9381 | val_loss=0.2105 val_acc=0.9282\n",
      "Epoch 06/12 | train_loss=0.1447 train_acc=0.9462 | val_loss=0.2063 val_acc=0.9282\n",
      "Epoch 07/12 | train_loss=0.1267 train_acc=0.9525 | val_loss=0.2081 val_acc=0.9278\n",
      "Epoch 08/12 | train_loss=0.1104 train_acc=0.9585 | val_loss=0.2026 val_acc=0.9328\n",
      "Epoch 09/12 | train_loss=0.0976 train_acc=0.9645 | val_loss=0.2152 val_acc=0.9305\n",
      "Epoch 10/12 | train_loss=0.0854 train_acc=0.9678 | val_loss=0.2279 val_acc=0.9302\n",
      "Epoch 11/12 | train_loss=0.0783 train_acc=0.9704 | val_loss=0.2282 val_acc=0.9340\n",
      "Epoch 12/12 | train_loss=0.0687 train_acc=0.9740 | val_loss=0.2382 val_acc=0.9322\n",
      "\n",
      "Saved best checkpoint: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/ckpt/model1_best.pt\n",
      "Saved training curves to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures\n",
      "Device: mps\n",
      "\n",
      "Test Accuracy: 0.9272\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top     0.9013    0.8680    0.8844      1000\n",
      "     Trouser     0.9959    0.9780    0.9869      1000\n",
      "    Pullover     0.8786    0.8970    0.8877      1000\n",
      "       Dress     0.9176    0.9460    0.9316      1000\n",
      "        Coat     0.8804    0.8830    0.8817      1000\n",
      "      Sandal     0.9850    0.9830    0.9840      1000\n",
      "       Shirt     0.8018    0.8010    0.8014      1000\n",
      "     Sneaker     0.9696    0.9570    0.9633      1000\n",
      "         Bag     0.9859    0.9820    0.9840      1000\n",
      "  Ankle boot     0.9578    0.9770    0.9673      1000\n",
      "\n",
      "    accuracy                         0.9272     10000\n",
      "   macro avg     0.9274    0.9272    0.9272     10000\n",
      "weighted avg     0.9274    0.9272    0.9272     10000\n",
      "\n",
      "Saved normalized confusion matrix to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/confusion_matrix_normalized_model1.png\n",
      "Device: mps\n",
      "Saved Figure 1 to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/sample_images_fashion_mnist_pytorch.png\n",
      "Saved filters to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/filters_conv1_model1.png\n",
      "Saved sample preview to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/sample_123_true_9_pred_9_model1.png\n",
      "Saved activation maps to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/activation_conv1_idx123_model1.png\n",
      "Saved activation maps to: /Users/E.M.A./Documents/fashionMNIST/model1_pytorch/outputs/figures/activation_conv2_idx123_model1.png\n",
      "Done. Check outputs/figures/ folder.\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(epochs=12)\n",
    "model, best_path, history, fig_dir = run_training(cfg)\n",
    "\n",
    "run_test_eval()\n",
    "run_visualizations(sample_idx=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857a91a-6480-44be-a31c-c27d828b2515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fashionMNIST)",
   "language": "python",
   "name": "fashionmnist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
