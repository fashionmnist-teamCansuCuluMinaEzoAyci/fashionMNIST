{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bd6aed-3f14-47f7-88e5-63041aba4fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    pd = None  \n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "LOSS_YLIM   = (0.0, 1.2)\n",
    "LOSS_YTICKS = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]\n",
    "\n",
    "ACC_YLIM    = (0.7, 1.0)\n",
    "ACC_YTICKS  = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237733f5-77f9-4076-945c-0582bbb59a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False) # no Bias because we 'll use batchNorm\n",
    "        self.bn = nn.BatchNorm2d(out_ch) #Normalize all feature maps\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x))) # Conv2d → BatchNorm2d → ReLU\n",
    "\n",
    "\n",
    "class Model2CNN(nn.Module):\n",
    "    \n",
    "    \"\"\" Fashion-MNIST CNN with BatchNorm, Dropout, GAP \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, dropout=0.20):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential( # A container that outputs each layer inside\n",
    "            ConvBNReLU(1, 32),\n",
    "            ConvBNReLU(32, 32),\n",
    "            nn.MaxPool2d(2),   # 28 -> 14\n",
    "            nn.Dropout2d(0.05),\n",
    "\n",
    "            ConvBNReLU(32, 64),\n",
    "            ConvBNReLU(64, 64),\n",
    "            nn.MaxPool2d(2),   # 14 -> 7\n",
    "            nn.Dropout2d(0.10),\n",
    "\n",
    "            ConvBNReLU(64, 128),\n",
    "            ConvBNReLU(128, 128), \n",
    "            nn.Dropout2d(0.15),\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.20),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    return Model2CNN(num_classes=10, dropout=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83427ee-c337-4e39-96a5-6ff2f314f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def ensure_dirs(base_dir: Path):\n",
    "    out_dir = base_dir / \"outputs\"\n",
    "    ckpt_dir = out_dir / \"ckpt\"\n",
    "    fig_dir = out_dir / \"figures\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir, ckpt_dir, fig_dir\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def save_grid(feature_maps, title, out_path, cols=8, figsize=(8, 4)):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n_maps = len(feature_maps)\n",
    "    rows = (n_maps + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_maps:\n",
    "            ax.imshow(feature_maps[i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.savefig(out_path, dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "def unnormalize(x):\n",
    "    return x * 0.5 + 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dd68f6-dc68-4988-8aa5-f23663f70487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_model2(base_dir: Path | None = None):\n",
    "\n",
    "    print(\"==== Training Model 2 ====\")\n",
    "\n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    _, ckpt_dir, fig_dir = ensure_dirs(base_dir)\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ])\n",
    "\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ])\n",
    "\n",
    "    data_root = base_dir / \"data\"\n",
    "\n",
    "    full_train = datasets.FashionMNIST(\n",
    "        root=data_root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_tf,\n",
    "    )\n",
    "\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(full_train) * val_ratio)\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_set, val_set = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer, scheduler\n",
    "    model = build_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 12\n",
    "    best_val_acc = 0.0\n",
    "    best_path = ckpt_dir / \"model2_best.pt\"\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, device, optimizer, criterion)\n",
    "        val_loss,   val_acc   = evaluate(model, val_loader, device, criterion)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | \"\n",
    "            f\"time={dt:.1f}s\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\"model_state\": model.state_dict(), \"val_acc\": best_val_acc}, best_path)\n",
    "\n",
    "    print(f\"\\nSaved best checkpoint to: {best_path}\")\n",
    "    \n",
    "    epochs_arr = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs_arr, history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs_arr, history[\"val_loss\"],   label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Model-2 Loss Curve\")\n",
    "    plt.ylim(LOSS_YLIM)\n",
    "    plt.yticks(LOSS_YTICKS)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / \"training_loss_model2.png\", dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs_arr, history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(epochs_arr, history[\"val_acc\"],   label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model-2 Accuracy Curve\")\n",
    "    plt.ylim(ACC_YLIM)\n",
    "    plt.yticks(ACC_YTICKS)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / \"training_accuracy_model2.png\", dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "    # Save metadata just like Model 1\n",
    "    meta = {\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"device\": str(device),\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    "    (ckpt_dir / \"train_meta_model2.json\").write_text(\n",
    "        json.dumps(meta, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"\\nSaved metadata: {ckpt_dir / 'train_meta_model2.json'}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4984c4e-33ea-467c-b967-245b03f0af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_full_evaluation_model2(base_dir: Path | None = None, ckpt_name: str = \"model2_best.pt\", batch_size: int = 256):\n",
    "\n",
    "    print(\"\\n==== Full Evaluation: Model 2 ====\\n\")\n",
    "\n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    _, ckpt_dir, fig_dir = ensure_dirs(base_dir)\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    \n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    test_set = datasets.FashionMNIST(\n",
    "        root=base_dir / \"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=tf\n",
    "    )\n",
    "    loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    \n",
    "    ckpt_path = ckpt_dir / ckpt_name\n",
    "    model = build_model().to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        preds = model(x).argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    acc = (all_labels == all_preds).mean()\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    (fig_dir / \"test_accuracy_model2.txt\").write_text(f\"{acc:.4f}\")\n",
    "\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=4)\n",
    "    print(report)\n",
    "    (fig_dir / \"classification_report_model2.txt\").write_text(report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                vmin=0.0, vmax=1.0)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.title(\"Normalized Confusion Matrix - Model 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / \"confusion_matrix_model2_normalized.png\", dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved normalized confusion matrix to: {fig_dir / 'confusion_matrix_model2_normalized.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e6a91d-7f3a-4272-93b7-a240dde79275",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_visualizations_model2(base_dir: Path | None = None,\n",
    "                              ckpt_name: str = \"model2_best.pt\",\n",
    "                              sample_idx: int = 123):\n",
    "\n",
    "    print(\"\\n==== Visualization Model 2 ====\\n\")\n",
    "\n",
    "    if base_dir is None:\n",
    "        try:\n",
    "            base_dir = Path(__file__).resolve().parent\n",
    "        except NameError:\n",
    "            base_dir = Path().resolve()\n",
    "\n",
    "    _, _, fig_dir = ensure_dirs(base_dir)\n",
    "    ckpt_path = base_dir / \"outputs\" / \"ckpt\" / ckpt_name\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    test_ds = datasets.FashionMNIST(\n",
    "        root=base_dir / \"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=tf\n",
    "    )\n",
    "\n",
    "    model = build_model().to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    idxs = list(range(10))\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        x, y = test_ds[idx]\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(unnormalize(x).squeeze(), cmap=\"gray\")\n",
    "        plt.title(CLASS_NAMES[int(y)], fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Sample Images - Model 2\")\n",
    "    plt.tight_layout()\n",
    "    out1 = fig_dir / \"sample_images_model2.png\"\n",
    "    plt.savefig(out1, dpi=120)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out1}\")\n",
    "\n",
    "    print(\"\\nExtracting STAGE-BASED activation maps...\\n\")\n",
    "\n",
    "    stage_layers = {\n",
    "        \"stage1\": 3,\n",
    "        \"stage2\": 7,\n",
    "        \"stage3\": 10\n",
    "    }\n",
    "\n",
    "    activations = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook_fn(module, inp, out):\n",
    "            activations[name] = out.detach().cpu()\n",
    "        return hook_fn\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    for stage_name, layer_idx in stage_layers.items():\n",
    "        h = model.features[layer_idx].register_forward_hook(get_activation(stage_name))\n",
    "        hooks.append(h)\n",
    "        print(f\"  ✔ Hook registered for {stage_name} at model.features[{layer_idx}]\")\n",
    "\n",
    "    x, y_true = test_ds[sample_idx]\n",
    "    x_batch = x.unsqueeze(0).to(device)\n",
    "    _ = model(x_batch)\n",
    "\n",
    "    h = model.features[1].register_forward_hook(get_activation(\"stage1_mid\"))\n",
    "    hooks.append(h)\n",
    "    print(\"Hook registered for stage1_mid at model.features[1]\")\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"\\nSaving activation map grids for each stage...\\n\")\n",
    "\n",
    "    for stage_name, act in activations.items():\n",
    "        act = act.squeeze(0).numpy()\n",
    "        maps = act[:32]\n",
    "\n",
    "        out_path = fig_dir / f\"activation_maps_{stage_name}_idx{sample_idx}.png\"\n",
    "        save_grid(\n",
    "            maps,\n",
    "            f\"Activation Maps - {stage_name.upper()} (Model 2)\",\n",
    "            out_path,\n",
    "            cols=8,\n",
    "        )\n",
    "        \n",
    "        print(f\"  → saved: {out_path}\")\n",
    "\n",
    "    print(\"\\nStage-based activation map visualizations completed.\\n\")\n",
    "    print(\"Check outputs/figures for results!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cafca5-46b7-4fcd-bb32-b55b12c783dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training Model 2 ====\n",
      "Device: mps\n",
      "Epoch 01/12 | train_loss=0.7750 train_acc=0.7158 | val_loss=0.4986 val_acc=0.8230 | time=14.9s\n",
      "Epoch 02/12 | train_loss=0.4659 train_acc=0.8289 | val_loss=0.3867 val_acc=0.8570 | time=9.9s\n",
      "Epoch 03/12 | train_loss=0.3977 train_acc=0.8560 | val_loss=0.3287 val_acc=0.8747 | time=9.9s\n",
      "Epoch 04/12 | train_loss=0.3576 train_acc=0.8695 | val_loss=0.3188 val_acc=0.8782 | time=9.6s\n",
      "Epoch 05/12 | train_loss=0.3296 train_acc=0.8800 | val_loss=0.3166 val_acc=0.8798 | time=9.5s\n",
      "Epoch 06/12 | train_loss=0.3096 train_acc=0.8880 | val_loss=0.2849 val_acc=0.8920 | time=9.5s\n",
      "Epoch 07/12 | train_loss=0.2966 train_acc=0.8929 | val_loss=0.2967 val_acc=0.8845 | time=9.6s\n",
      "Epoch 08/12 | train_loss=0.2814 train_acc=0.8977 | val_loss=0.2545 val_acc=0.9038 | time=9.6s\n",
      "Epoch 09/12 | train_loss=0.2733 train_acc=0.9016 | val_loss=0.2574 val_acc=0.9035 | time=10.7s\n",
      "Epoch 10/12 | train_loss=0.2639 train_acc=0.9033 | val_loss=0.2368 val_acc=0.9093 | time=10.0s\n",
      "Epoch 11/12 | train_loss=0.2576 train_acc=0.9075 | val_loss=0.2460 val_acc=0.9087 | time=9.9s\n",
      "Epoch 12/12 | train_loss=0.2510 train_acc=0.9101 | val_loss=0.2407 val_acc=0.9120 | time=9.6s\n",
      "\n",
      "Saved best checkpoint to: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/ckpt/model2_best.pt\n",
      "\n",
      "Saved metadata: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/ckpt/train_meta_model2.json\n",
      "\n",
      "==== Full Evaluation: Model 2 ====\n",
      "\n",
      "Device: mps\n",
      "\n",
      "Test Accuracy: 0.9137\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top     0.8435    0.8840    0.8633      1000\n",
      "     Trouser     0.9930    0.9860    0.9895      1000\n",
      "    Pullover     0.7819    0.9500    0.8578      1000\n",
      "       Dress     0.9025    0.9440    0.9228      1000\n",
      "        Coat     0.8969    0.8260    0.8600      1000\n",
      "      Sandal     0.9839    0.9770    0.9804      1000\n",
      "       Shirt     0.8558    0.6530    0.7408      1000\n",
      "     Sneaker     0.9617    0.9530    0.9573      1000\n",
      "         Bag     0.9841    0.9930    0.9886      1000\n",
      "  Ankle boot     0.9510    0.9710    0.9609      1000\n",
      "\n",
      "    accuracy                         0.9137     10000\n",
      "   macro avg     0.9154    0.9137    0.9121     10000\n",
      "weighted avg     0.9154    0.9137    0.9121     10000\n",
      "\n",
      "Saved normalized confusion matrix to: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/figures/confusion_matrix_model2_normalized.png\n",
      "\n",
      "==== Visualization Model 2 ====\n",
      "\n",
      "Device: mps\n",
      "Saved: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/figures/sample_images_model2.png\n",
      "\n",
      "Extracting STAGE-BASED activation maps...\n",
      "\n",
      "  ✔ Hook registered for stage1 at model.features[3]\n",
      "  ✔ Hook registered for stage2 at model.features[7]\n",
      "  ✔ Hook registered for stage3 at model.features[10]\n",
      "Hook registered for stage1_mid at model.features[1]\n",
      "\n",
      "Saving activation map grids for each stage...\n",
      "\n",
      "  → saved: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/figures/activation_maps_stage1_idx123.png\n",
      "  → saved: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/figures/activation_maps_stage2_idx123.png\n",
      "  → saved: /Users/E.M.A./Documents/fashionMNIST/model2_pytorch/outputs/figures/activation_maps_stage3_idx123.png\n",
      "\n",
      "Stage-based activation map visualizations completed.\n",
      "\n",
      "Check outputs/figures for results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_training_model2()\n",
    "run_full_evaluation_model2()\n",
    "run_visualizations_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e77a3-a3b7-4c45-9d7f-79bc34b9359b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fashionMNIST)",
   "language": "python",
   "name": "fashionmnist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
